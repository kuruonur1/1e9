Using gpu device 0: Tesla K20m
  toy  log                      hepoch    max_layers    fepoch    max_evals
    0  hepoch:50,bnorm:added        50             3       100          100
loading data...
loading data done.
opt       activation                          n_batch          hidden             bnorm
--------  ----------------------------------  ---------------  -----------------  -------
['adam']  ['sigmoid', 'tanh', 'relu', 'elu']  [128, 256, 512]  [512, 1024, 2048]  [0, 1]

   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2     dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  ------
32.6939        512  adam   sigmoid       0.0005  69.6773        1  2048  1024  2048  0.4847  0.6479  0.8846  0.4385
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
33.8424        256  adam   elu           0.0010  10.6959        0   512   512        0.6344  0.3192  0.4078
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2     dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  ------
33.9483        512  adam   tanh          0.0003  50.7219        0   512  2048  1024  0.5624  0.4303  0.5053  0.7308
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1    dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  -----  -----
32.0704        512  adam   relu          0.0003  10.6067        0   512              0.3287  0.3515
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
32.3207        512  adam   sigmoid       0.0006  52.4309        0   512  1024        0.6724  0.4531  0.1690
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2     dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  ------
34.9391        256  adam   tanh          0.0009  86.6770        0  1024   512  2048  0.5616  0.7294  0.3882  0.3929
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2     dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  ------
32.0467        128  adam   sigmoid       0.0007  57.0961        1  1024   512  2048  0.5159  0.1340  0.3155  0.8634
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2     dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  ------
35.4063        256  adam   tanh          0.0004  14.2489        0  1024   512  1024  0.8385  0.4800  0.7909  0.7816
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
32.7347        128  adam   elu           0.0002  65.0503        0  1024  2048        0.5819  0.6065  0.1792
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2     dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  ------
34.2353        256  adam   sigmoid       0.0005  78.1123        0  2048  1024  1024  0.8387  0.1065  0.3632  0.8852
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1    dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  -----  -----
35.1068        128  adam   tanh          0.0009  42.0089        1   512              0.8640  0.4117
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2     dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  ------
35.1404        128  adam   elu           0.0007  81.1592        0  1024   512   512  0.1995  0.6322  0.6693  0.8067
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2     dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  ------
33.7411        128  adam   elu           0.0003  66.1668        1   512  2048  2048  0.8442  0.1420  0.8712  0.5165
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
34.5772        512  adam   tanh          0.0004  15.4063        1   512  2048        0.8675  0.2384  0.8666
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2     dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  ------
33.3786        256  adam   sigmoid       0.0004  94.3906        0  2048  1024  1024  0.7780  0.1712  0.8134  0.5022
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2     dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  ------
32.8725        256  adam   sigmoid       0.0004  74.9593        1  2048   512  1024  0.8573  0.4559  0.4153  0.1237
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
30.9583        128  adam   sigmoid       0.0002  49.1471        1   512  1024        0.3194  0.2934  0.2845
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1    dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  -----  -----
31.7061        512  adam   elu           0.0001  74.8158        0  1024              0.1740  0.3565
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2     dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  ------
33.0101        256  adam   elu           0.0004  91.7468        1  1024   512   512  0.8013  0.6570  0.1403  0.2235
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1    dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  -----  -----
31.6717        128  adam   sigmoid       0.0008  90.2159        1  1024              0.3576  0.4831
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1    dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  -----  -----
34.1444        128  adam   relu          0.0008  33.2524        1  1024              0.6990  0.8555
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1    dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  -----  -----
32.2611        128  adam   sigmoid       0.0001  29.8705        1  2048              0.4590  0.7538
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
33.0694        128  adam   sigmoid       0.0008  27.4027        1  2048  1024        0.1091  0.8479  0.8372
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
30.4405        128  adam   sigmoid       0.0007  42.4423        1  1024  1024        0.1932  0.1092  0.5540
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
30.5957        128  adam   relu          0.0006  42.6120        1  1024  1024        0.2003  0.1067  0.5378
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
30.6495        128  adam   relu          0.0006  42.5855        1  1024  1024        0.1284  0.1086  0.6110
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
31.0477        128  adam   relu          0.0007  21.2248        1  1024  1024        0.2955  0.1002  0.6330
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
31.2539        128  adam   relu          0.0005  37.0772        1  1024   512        0.3049  0.1038  0.5181
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
32.3476        128  adam   relu          0.0005  58.5877        1  1024  1024        0.1894  0.7379  0.7059
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
30.5292        128  adam   relu          0.0007  41.1970        1  2048  1024        0.3950  0.2099  0.3877
   loss    n_batch  opt    activation        lr    norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  ------  -------  ----  ----  ----  ------  ------  ------  -----
30.8842        128  adam   relu          0.0009  5.6264        1  2048  1024        0.4437  0.4439  0.3736
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
30.5634        128  adam   relu          0.0007  25.6135        1  2048  2048        0.4255  0.2331  0.3958
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
32.1956        512  adam   relu          0.0007  99.6644        1  2048   512        0.4065  0.2134  0.7358
   loss    n_batch  opt    activation        lr    norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  ------  -------  ----  ----  ----  ------  ------  ------  -----
30.6187        512  adam   sigmoid       0.0010  2.2354        1  2048  1024        0.5228  0.3795  0.2641
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
33.6912        128  adam   tanh          0.0009  48.2915        1  2048  1024        0.7568  0.5797  0.4288
   loss    n_batch  opt    activation        lr     norm    bnorm    h1    h2    h3     dr0     dr1     dr2    dr3
-------  ---------  -----  ------------  ------  -------  -------  ----  ----  ----  ------  ------  ------  -----
30.1710        128  adam   relu          0.0006  21.7970        1  2048  1024        0.2561  0.2009  0.2914
